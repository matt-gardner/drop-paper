The success of reading comprehension on many existing benchmarks proved it effectiveness in extracting answers based on soft context matching and entity typing. A remarkable achievement is that several systems already outperform human on the SQuAD 1.1 \cite{Rajpurkar2016SQuAD10} dataset, which requires the machine to extract a span from the passage as the predicted answer. A large portion (???\%) of the answers in our dataset are also spans in the passage, but we expect that extracting these answers may require more sophisticated abilities than what the current systems can do.
% The success of reading comprehension on many existing benchmarks proved its effectiveness in aligning question with passage and extracting concise answer spans from the passage. 
% When constructing our dataset, we also allow a large portion of the answers (???\%) to be spans in the passage. However, we expect that such spans should not be extracted by simple lexical matching.
% Although our dataset is designed to avoid simple lexical matching, a large portion of the answers (???\%) are allowed to be spans in the passage, which could possibly be answered by SQuAD-style reading comprehension models. 

To test the real difficulty introduced by our dataset, we evaluate several representative SQuAD-style reading comprehension models on it, including:
(1) \textbf{BiDAF} \cite{Seo2016BidirectionalAF}, which is the adversarial baseline we used in data construction; 
(2) \textbf{QANet} \cite{yu2018qanet}, which is currently the best-performing published model on SQuAD 1.1 without data augmentation or pre-training; 
(3) \textbf{QANet + ELMo}, which enhances the QANet model by concatenating the pre-trained ELMo \cite{peters2018elmo} representations to the original embeddings; 
(4) \textbf{BERT} \cite{Devlin2018BERTPO}, which recently achieved significant improvement on many NLP tasks by a novel way of pre-training, and we adopt their reading comprehension model for SQuAD. These four systems achieved different levels of performance on SQuAD 1.1, with their EM scores on the development set as 66.8, 72.7, 78.1, 84.7\footnote{The first three scores are based on our own implementation, while last score for BERT is based on an open-source implementation from Hugging Face: https://github.com/huggingface/pytorch-pretrained-bert} respecitvely.

We trained these models on our dataset. As some answers in DROP cannot be represented as spans in the passage (???\%), we simply skipped them while training these SQuAD-style models. Another thing to be noted is that, different from SQuAD, our dataset only provides the answer strings instead of their spans in the passage. So, we adopted the marginal likelihood objective function proposed by \newcite{garder2018simplerc}, which sums over the probabilities of all the matching spans. For the BERT baseline, however, as we directly ran the open-sourced system from Hugging Face, we convert our training data to the format of SQuAD by treating the first matching span in the passage as the gold span. 