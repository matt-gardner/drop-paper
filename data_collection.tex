
We collect the dataset in three phases namely, extracting passage from Wikipedia, crowdsourcing question-answer pair collection and crowdsourcing a subset of questions in the dataset to obtain additional answers. 
\\
\textbf{Passage extraction}. To create questions that require arithmetic operations to answer, we sampled passages from Wikipedia  that have greater than twenty numbers in the passage. We oversampled from NFL game summaries and history articles in Wikipedia as they have a higher percentage of numerical values. \dheeru{Fix it later as it will be equally sampled}
\\
\textbf{Question-Answer collection}. We used Amazon Mechanical Turk to crowdsource the collection of question-answer pairs. We also setup a server hosting the BiDaF model to adversarialy create questions that can not be answered by a SQuAD-style model. The turkers were given 5 passages in each HIT and were asked to create a total of at least 12 questions. A question-answer pair was only accepted if the answer returned by the BiDaF model for that passage-question pair did not match the turker answer. Initially, we opened the HITs to all the turkers located in the United States. Gradually, as the turkers got acquainted with the task, we created a pool of good turkers and continued our dataset collection with these turkers. The turkers earned an average of \$10 per hour. Overall, the turkers seemed to enjoy the task and we received great reviews on TurkerView and in person.
\\
We provided sample questions from five chief database operation categories: Addition/Subtraction, Minimum/Maximum, Counting, Selection and Comparison to help the turkers focus on questions that need complex linguistic understanding and discrete reasoning. An answer could be of three types: span, date and number. A span is a continuous phrase from question or passage. An answer can have have multiple spans. The date-type answers were a part of history and other open-domain articles. A number-type answer is any real number. We restricted to ``how many \textit{[units]} " type questions for number answer. This helped us overlook specific units associated with each number making the evaluation simpler.
\\
We collected a total of 53,570 \dheeru{put the new value} question-answer pairs across all three domains: NFL, History and Open domain. The dataset was partitioned randomly into training (80\%), development(20\%) and test (20\%) sets, with mutually exclusive passages across the splits.


\textbf{Answer Validation}. 
To make the evaluation more robust, we additionally collected at least two answers for all the questions in development and test set via crowdsourcing. The turkers were shown passage-question pair and asked to answer the question. Additionally, they were given a checkbox to tick, if the question could not be answered. We found that the inter-annotator agreement was $\kappa=0.x$. \dheeru{Put correct value here and talk about it}. We, then, created a gold annotation set, with answers that at least two out of three turkers agreed on. \dheeru{What to do if there are full disagreement: check empirically and then write}. We also, collected expert annotations on 420 questions. The human performance measured against the gold set was 95.0\% accuracy (exact-match) and 96.3\% $F_1$\dheeru{update the numbers with new validation results}. x\% of questions were marked unanswerable and were removed from the dataset.



