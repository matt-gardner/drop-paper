
We collect the dataset in three phases which include, extracting passage from Wikipedia, crowdsourcing question-answer pair collection and crowdsourcing collection of additional answers on a subset of questions. 
\\
\textbf{Passage extraction}. To focus on questions that require discrete reasoning to answer, we sampled passages from Wikipedia that have more than twenty numbers in the passage. We over-sampled from National Football league (NFL) game summaries and history articles in Wikipedia as they have a higher percentage of numerical values.
%\dheeru{Fix it later as it will be equally sampled}
\\
\textbf{Question-Answer collection}. We used Amazon Mechanical Turk to crowdsource the collection of question-answer pairs. We also setup a server hosting the BiDAF model to adversarially create questions that can not be answered by a SQuAD-style model. We chose BiDAF, as at the time we started a pilot for this task in May 2018, its performance on SQuAD 1.1 was very close to the state of the art and Allen-NLP\footnote{https://github.com/allenai/allennlp} provides an easy and robust way to setup a BiDAF server. The turkers were given 5 passages in each HIT and were asked to create a total of at least 12 questions. A question-answer pair was only accepted if the answer returned by the BiDAF model for that passage-question pair did not match the turker answer. Initially, we opened the HITs to all the turkers located in the United States. Gradually, as the turkers got acquainted with the task, we created a pool of good turkers and continued our dataset collection with these turkers. The turkers earned an average of \$10 per hour. Overall, the turkers seemed to enjoy the task and we received great reviews on TurkerView and in person.
\\
We provided sample questions from five chief semantic parsing operation categories: Addition/Subtraction, Minimum/Maximum, Counting, Selection and Comparison in the HIT to help the turkers focus on questions that need complex linguistic understanding and discrete reasoning. An answer could be of three types: span, date and number. A span is a continuous phrase from question or passage. An answer can have have multiple spans. The date-type answers were a part of history and other open-domain articles. A number-type answer is any real number. We restricted to ``how many \textit{[units]} " type questions for number answer. This helped us overlook specific units associated with each number making the evaluation simpler.
\\
We collected a total of 55,463 question-answer pairs across all three domains: NFL, History and Open domain. The dataset was partitioned randomly into training (80\%), development(10\%) and test (10\%) sets, with mutually exclusive passages across the splits.


\textbf{Answer Validation}. 
To make the evaluation more robust, we additionally collected at least two answers for all the questions in development and test set via crowdsourcing. The turkers were shown passage-question pair and asked to answer the question. Additionally, they were given a checkbox to tick, if the question could not be answered. We found that the inter-annotator agreement was $\kappa=0.74$ (0.81 for number, 0.62 for spans, 0.65 for dates). We, then, created a gold annotation set, with answers that at least two out of three turkers agreed on. We also, collected expert annotations on 420 questions. The human performance measured against the gold set was 95.0\% accuracy (exact-match) and 96.3\% $F_1$. 0.7\% of questions were marked unanswerable and were removed from the dataset.



