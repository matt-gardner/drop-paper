\textbf{Question answering datasets:} With systems reaching human performance on the Stanford Question Answering Dataset (SQuAD) \citep{Rajpurkar2016SQuAD10}, many follow-on tasks are currently being proposed.  All of these datasets throw in additional complexities to the reading comprehension challenge, around tracking conversational state~\citep{Reddy2018CoQAAC,Choi2018QuACQA}, requiring passage retrieval~\citep{Joshi2017TriviaQAAL,Yang2018HotpotQAAD}, mismatched passages and questions~\citep{Saha2018DuoRCTC,Kocisk2018TheNR,Rajpurkar2018KnowWY}, integrating knowledge from external sources~\citep{Mihaylov2018CanAS,Zhang2018ReCoRDBT}, or a particular kind of ``multi-step'' reasoning over multiple documents~\citep{Welbl2018ConstructingDF,Khashabi2018LookingBT}.  We applaud these efforts, which offer good avenues to study these additional phenomena.  However, we are concerned with \emph{paragraph understanding}, which on its own is far from solved, so DROP has none of these additional complexities.  It consists of single passages of text paired with independent questions, with only linguistic facility required to answer the questions.\footnote{Some questions in our dataset require limited sports domain knowledge to answer; we expect that there are enough such questions that systems can reasonably learn this knowledge from the data.}  One could argue that we are adding numerical reasoning as an ``additional complexity'', and this is true; however, it is only simple reasoning that is relatively well-understood in the semantic parsing literature, and we use it as a necessary means to force more comprehensive passage understanding.

Many existing algebra word problem datasets also contain similar phenomena to what is in DROP~\citep{KoncelKedziorski2015ParsingAW,Ling2017ProgramIB}.  Our dataset is different in that it typically has much longer contexts, is more open domain, and requires deeper paragraph understanding.
  
\textbf{Semantic parsing:} The semantic parsing literature has a long history of trying to understand complex, compositional question semantics in terms of some grounded knowledge base or other environment~\citep[\emph{inter alia}]{Zelle1996LearningTP,Zettlemoyer2005LearningTM,Berant2013SemanticPO}.  It is this literature that we modeled our questions on, particularly looking at the questions in the WikiTableQuestions dataset~\citep{Pasupat2015CompositionalSP}.  If we had a structured, tabular representation of the content of our paragraphs, DROP would be largely the same as WikiTableQuestions, with similar (possibly even simpler) question semantics.  Our novelty is that we are the first to combine these complex questions with paragraph understanding, with the aim of encouraging systems that can produce comprehensive structural analyses of paragraphs, either explicitly or implicitly.

\textbf{Adversarial dataset construction:} We continue a recent trend in creating datasets with adversarial baselines in the loop~\citep{Paperno2016TheLD,Minervini2018AdversariallyRN,Zellers2018SWAGAL,Zhang2018ReCoRDBT,zellers2018VCR}.  In our case, instead of using an adversarial baseline to filter automatically generated examples, we use it in a crowdsourcing task, to raise the difficulty level of the questions provided by crowd workers.

\textbf{Neural symbolic reasoning:} DROP is designed to encourage research on methods that combine neural methods with discrete, symbolic reasoning.  We present one such model in \secref{model}.  Other related work along these lines has been done by \citet{Reed2015NeuralP}, \citet{Neelakantan2015NeuralPI}, and \citet{Liang2017NeuralSM}.