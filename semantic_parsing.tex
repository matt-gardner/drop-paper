%% \begin{itemize}
%%     \item Motivation - Predicate argument structure were developed in order to capture "meaning" of a sentence / proxy for information extraction, + brief discussion on the representation we experimented with.
%%     \item Predicate-argument structure representation.
%%     \item example.
%%     \item query language (mention that it's similar to other query languages).
%%     \item context representation.
%%     \item Model used 
%%     \item hyperparameters (tree depth,...)
%% \end{itemize}

Semantic parsing deals with translating natural language utterances to
into executable formal language. For tasks requiring answering questions in context, semantic parsing
has been used to parse representations that can be
executed against some structured representation of the context, such as @@
This setup allows for complex reasoning over contextual knowledge, and has
been successfully used in several natural language understanding
problems~\citep[among others]{berant2013semantic,Yin2017ASN,chen2011learning}.
This paradigm was shown to particularly lend itself to constrained question-answering
over domain-specific datasets, such as geography (e.g., GeoQuery \cite{geoquery})
@@, or more recently @@ \gabi{more examples}.
Since many of \drop's questions require similar discrete reasoning, it is appealing
to port some of the successful work in semantic parsing to the \drop dataset.

\gabi{TODO: Pradeep, please take a look}
To that end, we will use the WikiTables parser \cite{Krishnamurthy2017neuralsp},
which populates a table from the input paragraph, and
parses the question as a query into that table.
The answer for the question is then deterministically computed given the predicted table
and query.

As \drop~ was constructed over paragraphs from various domains,
we experimented with several sentence representation formalisms which were
developed with the goal of capturing meaning in open-domain text.
Specifically, we will experiment with three prominent formalisms, each
representing different level of semantic granularity:
(1) syntactic dependencies \cite{sd}, which capture word-level relations,
(2) Open Information Extraction (Open IE; \cite{oie}), a shallow
form of semantic representation, which directly links predicates
with their arguments, abstracting away certain syntactic variations, and
(3) Semantic Role Labeling (SRL; \cite{srl}), which can be seen as an
augmentation of Open IE with predicate sense disambiguation and finer, predicate-specific
argument roles.

Each of these representations can be used to populate a table for WikiTables,
where lines correspond to predicate-argument structures,
and columns correspond to different argument types.
See \figref{semantic-parsing} for an example of the different predicate-argument
representations over an input sentence, and their corresponding tables.

Finally, we use the method described in \cite{Krishnamurthy2017neuralsp}
to train a solver to predict a logical form, and consequently an answer, from questions and tables.

\begin{table}[]
\begin{tabular}{@{}lllll@{}}
\toprule
Representation         & \#Rels & P & R & F1 \\ \midrule
Syntactic dependencies &             &           &        &    \\
Open IE                &             &           &        &    \\
Semantic Role Labeling  &             &           &        &    \\ \bottomrule
\end{tabular}
\end{table}
