%% TODO- GABI

%% \begin{itemize}
%% \item
%%   Look at the dev part, and come up with QA templates
%%   *which can be automatically detected*.
%% \item
%%   Assess their frequency on the dev set. How much of it do the chosen templates cover.
%%   (maybe this can be a part of the analysis section?)
%% \item
%%   Come up with heuristics for each QA template, probably based on frequencies in
%%   the dev set.
%%   How well do they do?
%% \end{itemize}

A recent line of work \cite{Gururangan:2018,Kaushik2018HowMR} has identified that popular synthetic NLP datasets
(such as SQuAD \cite{Rajpurkar2016SQuAD10} or SNLI \cite{Bowman2015ALA}) are prone to have artifacts and annotation biases which may be exploited by (advertently or inadvertently) by machine learning algorithms that
learn to pick up these artifacts as signal,
rather than actually learning something semantically meaningful.
These works have highlighted ways in which these artifcats can be exploited by models which explicitly focus on these biases, for example, by
 looking only at the question or the hypothesis in QA and entailment dataset, respectively.
Consequently, the resulting models were shown to be brittle and extremely sensitive
to minor data perturbations \cite{Glockner2018BreakingNS,others}.

While \drop~ was annotated in an adversarial setting
(see \secref{data_collection}) in an attempt to mitigate these effects, some
artifacts may still exist in the annotated data.
We try to estimate their extent by taking an adversarial
stance which tries to ``break'' \drop in some of the ways 
which were found to be successful on previous datasets.

\paragraph{Question-only baseline}

\paragraph{Paragraph-only baseline}

\paragraph{Majority baseline}
