%% TODO- GABI

%% \begin{itemize}
%% \item
%%   Look at the dev part, and come up with QA templates
%%   *which can be automatically detected*.
%% \item
%%   Assess their frequency on the dev set. How much of it do the chosen templates cover.
%%   (maybe this can be a part of the analysis section?)
%% \item
%%   Come up with heuristics for each QA template, probably based on frequencies in
%%   the dev set.
%%   How well do they do?
%% \end{itemize}

A recent line of work \cite{Gururangan:2018,Kaushik2018HowMR} has identified that popular synthetic NLP datasets
(such as SQuAD \cite{Rajpurkar2016SQuAD10} or SNLI \cite{Bowman2015ALA}) are prone to have artifacts and annotation biases which can be exploited by supervised algorithms that
learn to pick up these artifacts as signal instead of more meaningful semantic features.
% These works have highlighted ways in which these artifacts can be exploited by models which explicitly focus on these biases, for example, by
%  looking only at the question or the hypothesis in QA and entailment dataset, respectively.
% The resulting models were shown to be brittle and extremely sensitive
% to minor data perturbations \cite{Glockner2018BreakingNS,others}.
In the following, we try to identify such artifacts which may still exist in \drop, depite our adversarial annotation protocol.
% (see \secref{data_collection}) in an attempt to mitigate these effects, 
% In the following we try to ``break'' \drop~  ways 
% which were found to be successful on previous datasets.

\paragraph{Question-only and paragraph-only baselines:}
We estimate artifacts in both question and paragraph by training the QANET model described in \secref{rc} on a version of \drop~ where either the question or the 
paragraph input representation vectors are zeroed out.
Consequently, the resulting model can then only predict answer spans from either the question or the paragraph.

\paragraph{Majority baseline:}
We devise an adversarial baseline which tries to 
estimate the answer variance in \drop.
We start by counting the unigram and bigram development answer frequency 
per each  wh question-word (taken as the first word in the question). 
The \emph{majority baseline} then predicts an answer as the set of $n$ most common answer spans for the input question word.
